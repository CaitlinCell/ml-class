\section{Data Set and Feature Selection}
%\cite*{mimno-mccallum-11}

\subsection{Dataset Construction}
The United States Patent Office makes bulk patent grant information available through several private companies\nocite{USPTO:2013:patent-catalog} in XML format\nocite{USPTO:2013:dtd}. For our experiments we drew samples from patent grants made between May 1st 2012 and July 31st 2012. Over these three months the USPTO granted a total of 76,317 patents, or roughly 6,000 per week. We constructed two datasets from this, \emph{even-3500} and \emph{jagged-20000}. \emph{even-3500} consisted of 350 training and 150 test instances from 8 of the 9 CPC patent classes (one class, D, which corresponds to textiles and paper could not be used since a only 47 patents were granted to that section in our dataset). \emph{jagged-20000} was simply a random sample of 20,000 patents granted within the given timeframe, using all 9 of the CPC patent classes.

\subsection{Feature Selection}
Patents are semi-structured. That is, rather than simply being free text, there are certain fields that all patent grants are guaranteed to have (as well as other, optional fields). This needs to be taken into consideration when designing features for patent classification. Although a domain expert could potentially design features that were powerful, we ended up considering two relatively general feature sets, based on different sets of patent fields. One feature set simply considered the patent's description field using a unigram bag-of-words model. The other looked at the (smaller) title, abstract, and claims fields, but considered both unigram and trigram token features. In the bag-of-words model, each document is tokenized, and stopwords (words that are likely to appear in every document and will thus not provide any information) were removed. We used the NLP pipeline in FACTORIE to tokenize the patents and remove stopwords. We also added domain-specific stopwords based on observations of patent documents. 
\indent

Both of these bag of n-grams models can be easily translated into vectors. This allows us to use a variety of standard machine learning multiclass classification techniques. For some set of documents $\mathbf{D}$, let $\| \mathbf{V} \|$ denote the number of unique n-grams that appear in that set of documents (for a given tokenizer and stopword lexicon). Then we can represent each document $ d \in \mathbf{D}$ by a binary vector $\mathbf{f} \in \{0,1\}^{\| \mathbf{V} \|}$, where $\mathbf{f}_i = 1$ when $d$ contains the $i$\textsuperscript{th} n-gram in the vocabulary and 0 otherwise.


%Our data is from the United States Patent and Trademark Office Bulk Downloads located on Google.  The original data file is prepared as an XML file, which we then parse to find a list of words contained within the patent.  Based on the total list of words of all patents in a sample, a feature vector is created such that the feature vector includes a feature for every seen word within the dataset.  A 1 is included for a feature within a feature vector for a patent if the patent contains the word, and a 0 is included for a specific feature if the patent did not include the word.  This bag of words technique was used for all experiments.